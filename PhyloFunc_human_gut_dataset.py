# This code calculates the PhyloFunc distance for the human gut microbiome dataset.
# The code includes 4 Steps: data preprocessing, construction of branches, creation of extended taxon composition table, and PhyloFunc distance calculation  
# Original datafiles in the folder 1_datasets_search_results\3_human gut microbiome:

# Step1: preprocess the raw data to create a taxon-function table
# proteinGroups.txt, taxon.tsv, function.csv are generated by Metalab through searching the database
import pandas as pd
from collections import Counter
# Match files proteinGroups.txt,taxon.tsv, and function.csv to obtain taxon-function table
# Read proteinGroups.txt file and exclude any protein group with "REV_" in the ID names 
protein_group = pd.read_table("proteinGroups.txt", sep = "\t")
protein_group = protein_group[~protein_group['Protein IDs'].str.contains('REV_')]
# Expand protein groups by a semicolon
# Split the values in the "Protein Group accession" column by semicolons and then explode the dataframe
protein_group['Protein IDs'] = protein_group['Protein IDs'].str.split(';') 
protein_group = protein_group.explode('Protein IDs')  
protein_group_filter = protein_group.filter(regex='^(id|Protein IDs|.*ntensity.*)$')
protein_group_filter=protein_group_filter.drop("Intensity",axis=1)
protein_group_filter.columns = [col.replace(' ', '.') for col in protein_group_filter.columns]
# Read the taxon table and function table
original = pd.read_table("taxon.tsv", sep = "\t")
original=original[["user_genome","classification"]]
cog_function=pd.read_csv("function.csv", sep = ',')
cog_function = cog_function.dropna(subset=['Protein ID'])
cog_function['genome_ID'] = cog_function['Protein ID'].str.extract(r'(.*?bin.*?)_')
# Match taxon and function table to create a taxon-function table
merged_taxon_cog= cog_function.merge(original, left_on='genome_ID', right_on='user_genome', how='inner')
merged_taxon_cog = merged_taxon_cog.drop("user_genome",axis=1)
# Split values by semicolons
split_columns = merged_taxon_cog['classification'].str.split(';', expand=True)  
merged_taxon_cog_split = pd.concat([merged_taxon_cog, split_columns], axis=1)
merged_taxon_cog_split.columns = merged_taxon_cog_split.columns.tolist()[:-7] + ['d','p','c','o','f', 'g', 's']

# Merge the taxon_cog table with the protein data.
merge_proteingroup=protein_group_filter.merge(merged_taxon_cog_split, left_on='Protein.IDs', right_on='Protein ID', how='inner')
merge_proteingroup_tidy=merge_proteingroup.drop(["classification","Protein.IDs"],axis=1)
merge_proteingroup_tidy=merge_proteingroup_tidy.rename(columns={'id':'ProteinGroupIDs'})
protein_group_LFQ_intensity=merge_proteingroup_tidy.filter(regex='^(?!.*Intensity).*$', axis=1)
protein_group_Intensity=merge_proteingroup_tidy.filter(regex='^(?!.*LFQ).*$', axis=1)

# Change column name of sample from table meta_data.csv
meta_data=pd.read_csv("meta_data.csv", sep = ',')
meta_data_1=meta_data[["Name","Peptide.Name"]]
meta_data_2=meta_data[["Name","Protein.Name"]]
meta_data_1.rename(columns={"Peptide.Name": "sample_name"}, inplace=True)
meta_data_2.rename(columns={"Protein.Name": "sample_name"}, inplace=True)
sample_name= pd.concat([meta_data_1, meta_data_2], ignore_index=True)
# Rename the LFQ_intensity columns
for column in protein_group_LFQ_intensity.columns:
    new_name=sample_name.loc[sample_name["sample_name"]==column,"Name"].values
    if len(new_name) > 0:
        protein_group_LFQ_intensity.rename(columns={column:new_name[0]}, inplace=True)       
# Extract LFQintensity lines on the species level
Group_ID_species =  protein_group_LFQ_intensity[['ProteinGroupIDs', 's']].reset_index(inplace=False).drop(columns='index')
# Remove duplicated rows
LCA_species = Group_ID_species.drop_duplicates() 
# Extract LFQintensity lines on the species level
LCA_species_key = pd.DataFrame.from_dict(Counter(LCA_species['ProteinGroupIDs']).keys(), orient='columns')
LCA_species_number = pd.DataFrame.from_dict(Counter(LCA_species['ProteinGroupIDs']).values(), orient='columns')
LCA_species_count_list = pd.concat([LCA_species_key, LCA_species_number], axis=1)
LCA_species_count_list.columns = ['Group_ID', 'Count']
Unique_LCA_species_level = LCA_species_count_list.loc[LCA_species_count_list['Count'] == 1]
Protein_species_unique = protein_group_LFQ_intensity.merge(Unique_LCA_species_level, left_on='ProteinGroupIDs', right_on='Group_ID')
Protein_species_unique = Protein_species_unique[pd.notnull(Protein_species_unique['s'])]
Protein_species_unique=Protein_species_unique[Protein_species_unique['s']!='s__']
Protein_species_unique['LCA'] = "species"
Protein_species_unique_index = Protein_species_unique['ProteinGroupIDs']
merge_table_noSpeciesLCA = protein_group_LFQ_intensity[~protein_group_LFQ_intensity['ProteinGroupIDs'].isin(Protein_species_unique_index)]
# Extract LFQintensity lines on the genus level
Group_ID_genus =  merge_table_noSpeciesLCA[['ProteinGroupIDs', 'g']].reset_index(inplace=False).drop(columns='index')
LCA_genus = Group_ID_genus.drop_duplicates()  # remove if an id matches to the same genus
LCA_genus_key = pd.DataFrame.from_dict(Counter(LCA_genus['ProteinGroupIDs']).keys(), orient='columns')
LCA_genus_number = pd.DataFrame.from_dict(Counter(LCA_genus['ProteinGroupIDs']).values(), orient='columns')
LCA_genus_count_list = pd.concat([LCA_genus_key, LCA_genus_number], axis=1)
LCA_genus_count_list.columns = ['Group_ID', 'Count']
Unique_LCA_genus_level = LCA_genus_count_list.loc[LCA_genus_count_list['Count'] == 1]
Protein_genus_unique = merge_table_noSpeciesLCA.merge(Unique_LCA_genus_level, left_on='ProteinGroupIDs', right_on='Group_ID')
Protein_genus_unique = Protein_genus_unique[pd.notnull(Protein_genus_unique['g'])]
Protein_genus_unique=Protein_genus_unique[Protein_genus_unique['g']!='g__']
Protein_genus_unique['LCA'] = "genus"
len(Protein_genus_unique['ProteinGroupIDs'].drop_duplicates())
Protein_genus_unique_index = Protein_genus_unique['ProteinGroupIDs']
merge_table_noGenusLCA = merge_table_noSpeciesLCA[~merge_table_noSpeciesLCA['ProteinGroupIDs'].isin(Protein_genus_unique_index)]
len(merge_table_noGenusLCA)
# Extract LFQintensity lines on the family level
Group_ID_family =  merge_table_noGenusLCA[['ProteinGroupIDs', 'f']].reset_index(inplace=False).drop(columns='index')
LCA_family = Group_ID_family.drop_duplicates() 
LCA_family_key = pd.DataFrame.from_dict(Counter(LCA_family['ProteinGroupIDs']).keys(), orient='columns')
LCA_family_number = pd.DataFrame.from_dict(Counter(LCA_family['ProteinGroupIDs']).values(), orient='columns')
LCA_family_count_list = pd.concat([LCA_family_key, LCA_family_number], axis=1)
LCA_family_count_list.columns = ['Group_ID', 'Count']
Unique_LCA_family_level = LCA_family_count_list.loc[LCA_family_count_list['Count'] == 1]
Protein_family_unique = merge_table_noGenusLCA.merge(Unique_LCA_family_level, left_on='ProteinGroupIDs', right_on='Group_ID')
Protein_family_unique = Protein_family_unique[pd.notnull(Protein_family_unique['f'])]
Protein_family_unique=Protein_family_unique[Protein_family_unique['f']!='f__']
Protein_family_unique['LCA'] = "family"
len(Protein_family_unique['ProteinGroupIDs'].drop_duplicates())
Protein_family_unique_index = Protein_family_unique['ProteinGroupIDs']
merge_table_noFamilyLCA = merge_table_noGenusLCA[~merge_table_noGenusLCA['ProteinGroupIDs'].isin(Protein_family_unique_index)]

## Extract LFQintensity lines on the order level
Group_ID_order =  merge_table_noFamilyLCA[['ProteinGroupIDs', 'o']].reset_index(inplace=False).drop(columns='index')
LCA_order = Group_ID_order.drop_duplicates()  
LCA_order_key = pd.DataFrame.from_dict(Counter(LCA_order['ProteinGroupIDs']).keys(), orient='columns')
LCA_order_number = pd.DataFrame.from_dict(Counter(LCA_order['ProteinGroupIDs']).values(), orient='columns')
LCA_order_count_list = pd.concat([LCA_order_key, LCA_order_number], axis=1)
LCA_order_count_list.columns = ['Group_ID', 'Count']
Unique_LCA_order_level = LCA_order_count_list.loc[LCA_order_count_list['Count'] == 1]
Protein_order_unique = merge_table_noFamilyLCA.merge(Unique_LCA_order_level, left_on='ProteinGroupIDs', right_on='Group_ID')
Protein_order_unique = Protein_order_unique[pd.notnull(Protein_order_unique['o'])]
Protein_order_unique=Protein_order_unique[Protein_order_unique['o']!='o__']
Protein_order_unique['LCA'] = "order"
len(Protein_order_unique['ProteinGroupIDs'].drop_duplicates())
Protein_order_unique_index = Protein_order_unique['ProteinGroupIDs']
merge_table_noOrderLCA = merge_table_noFamilyLCA[~merge_table_noFamilyLCA['ProteinGroupIDs'].isin(Protein_order_unique_index)]
# Extract LFQintensity lines on the class level
Group_ID_class =  merge_table_noOrderLCA[['ProteinGroupIDs', 'c']].reset_index(inplace=False).drop(columns='index')
LCA_class = Group_ID_class.drop_duplicates()  # remove if an id matches to the same class
LCA_class_key = pd.DataFrame.from_dict(Counter(LCA_class['ProteinGroupIDs']).keys(), orient='columns')
LCA_class_number = pd.DataFrame.from_dict(Counter(LCA_class['ProteinGroupIDs']).values(), orient='columns')
LCA_class_count_list = pd.concat([LCA_class_key, LCA_class_number], axis=1)
LCA_class_count_list.columns = ['Group_ID', 'Count']
Unique_LCA_class_level = LCA_class_count_list.loc[LCA_class_count_list['Count'] == 1]
Protein_class_unique = merge_table_noOrderLCA.merge(Unique_LCA_class_level, left_on='ProteinGroupIDs', right_on='Group_ID')
Protein_class_unique = Protein_class_unique[pd.notnull(Protein_class_unique['c'])]
Protein_class_unique=Protein_class_unique[Protein_class_unique['c']!='c__']
Protein_class_unique['LCA'] = "class"
len(Protein_class_unique['ProteinGroupIDs'].drop_duplicates())
Protein_class_unique_index = Protein_class_unique['ProteinGroupIDs']
merge_table_noClassLCA = merge_table_noOrderLCA[~merge_table_noOrderLCA['ProteinGroupIDs'].isin(Protein_class_unique_index)]
# Extract LFQintensity lines on the phylum level
Group_ID_phylum =  merge_table_noClassLCA[['ProteinGroupIDs', 'p']].reset_index(inplace=False).drop(columns='index')
Group_ID_phylum['p'] = Group_ID_phylum['p'].str.split('_').str.get(2)
Group_ID_phylum['p'] = Group_ID_phylum['p'].apply(lambda x: 'p__' + str(x))
LCA_phylum = Group_ID_phylum.drop_duplicates() # remove if an id matches to same phylum
LCA_phylum_key = pd.DataFrame.from_dict(Counter(LCA_phylum['ProteinGroupIDs']).keys(), orient='columns')
LCA_phylum_number = pd.DataFrame.from_dict(Counter(LCA_phylum['ProteinGroupIDs']).values(), orient='columns')
LCA_phylum_count_list = pd.concat([LCA_phylum_key, LCA_phylum_number], axis=1)
LCA_phylum_count_list.columns = ['Group_ID', 'Count']
Unique_LCA_phylum_level = LCA_phylum_count_list.loc[LCA_phylum_count_list['Count'] == 1]
Protein_phylum_unique = merge_table_noClassLCA.merge(Unique_LCA_phylum_level, left_on='ProteinGroupIDs', right_on='Group_ID')
Protein_phylum_unique = Protein_phylum_unique[pd.notnull(Protein_phylum_unique['p'])]
Protein_phylum_unique=Protein_phylum_unique[Protein_phylum_unique['p']!='p__']
Protein_phylum_unique['LCA'] = "phylum"
len(Protein_phylum_unique['ProteinGroupIDs'].drop_duplicates())
Protein_phylum_unique_index = Protein_phylum_unique['ProteinGroupIDs']
merge_table_nophylumLCA = merge_table_noClassLCA[~merge_table_noClassLCA['ProteinGroupIDs'].isin(Protein_phylum_unique_index)]

# Merge all levels to get the taxon-function table
Protein_phylum_unique_index = Protein_phylum_unique['ProteinGroupIDs']
Protein_LCA_table = pd.concat([Protein_species_unique, Protein_genus_unique, Protein_family_unique, 
                               Protein_order_unique, Protein_class_unique, Protein_phylum_unique])
Protein_LCA_table=Protein_LCA_table.drop(columns=["Protein ID"])
Protein_LCA_table=Protein_LCA_table.drop_duplicates()
Protein_LCA_table.to_csv("Protein_LCA_table_V49_LFQ.csv")
Protein_LCA_species = Protein_LCA_table[(Protein_LCA_table['LCA']=="species")]
Protein_LCA_species_tidy=Protein_LCA_species.drop(columns=["COG_category","Group_ID","Count",'d','p','c','o','f', 'g','LCA','genome_ID'])
Protein_LCA_species_tidy.to_csv("Protein_LCA_table_V49_species_LFQ.csv")
Protein_LCA_species_tidy=Protein_LCA_species_tidy.drop(columns=["ProteinGroupIDs"])
Protein_LCA_species_COG_sum = Protein_LCA_species_tidy.groupby(['s', 'COG_number']).sum().reset_index()
Protein_LCA_species_COG_sum['s'] = Protein_LCA_species_COG_sum['s'].str.replace('s__', 's_')
Protein_LCA_species_COG_sum['s'] = Protein_LCA_species_COG_sum['s'].str.replace(' ', '_')
Protein_LCA_species_COG_sum = Protein_LCA_species_COG_sum.rename(columns={"s": "taxon"})
Protein_LCA_species_COG_sum.to_csv("Protein_LCA_species_COG_sum_V49_LFQ.csv",index=False)

# Step2: Construct the tree branches table
# The phylogenetic tree file V49-Midpoint-rooted-Tree.NWK was constructed by GTDB-Tk
from Bio import Phylo
import pandas as pd
import csv
# Read the phylogenetic tree file and the file containing the corresponding tree node names.
tree = Phylo.read("V49-Midpoint-rooted-Tree.NWK", "newick")
taxon_name_change_table=pd.read_csv("taxon_name_change_table_species.csv", sep = ',')
# Name internal nodes of the phylogenetic tree
def assign_names(tree):
    node_count = 0
    for clade in tree.find_clades(order='postorder'): 
        if not clade.name:  
            node_count += 1
            clade.name = f"Node{node_count}"
    return tree
tree_with_names = assign_names(tree)
# Rename the leaves nodes of the phylogenetic tree
def rename_leaf_nodes(node, rename_dict):
    if node.is_terminal():
        old_name = node.name
        if old_name in rename_dict["name"].tolist():
            new_name = rename_dict.loc[rename_dict['name'] == old_name, 'new_name'].iloc[0]
            node.name = new_name
        else:
            node.name='None'
    else:
        for child in node.clades:
            rename_leaf_nodes(child, rename_dict)
    return tree
# Process a phylogenetic tree clade to obtain information about its branches, including the predecessor, successor, and branch length.
def generate_branches(clade):
    branches = []
    for child in clade.clades:
        if child.branch_length is not None:
            if clade.name:
                branches.append(clade.name)  
            else:
                branches.append(clade.clades[0].name)  
            if child.name:
                branches.append(child.name)  
            else:
                branches.append(child.clades[0].name)  
            branches.append(child.count_terminals())  
            branches.append(child.branch_length)  
    return branches
# Create branch information
def collect_branches(clade):
    if clade.is_terminal():
        return 
    branches = generate_branches(clade)
    all_branches.append(branches)
    for child_clade in clade.clades:
        collect_branches(child_clade)
all_branches = []
rename_tree=rename_leaf_nodes(tree_with_names.root,taxon_name_change_table)  
collect_branches(rename_tree.root)   
with open("tree_rename.csv", mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["Precedent", "consequent", "length"])
    for branches in all_branches:
        if len(branches)==12:
            writer.writerow(branches[8:12])
        writer.writerow(branches[0:4])
        writer.writerow(branches[4:8])
branch=pd.read_csv("tree_rename.csv", sep = ',')
# Step3: calculate tax_composition, function composition and weighted_function_composition table according to Taxon-Function Table
# Read the preprocessed taxon-function file "Protein_LCA_species_COG_sum_V49_LFQ.csv" generated from Step1.
V49_data=pd.read_csv("Protein_LCA_species_COG_sum_V49_LFQ.csv", sep = ',')
# Calculate tax_composition table
column_A = 'taxon'  
column_Bs = V49_data.columns.tolist()[2:]
tax_composition = pd.DataFrame()  
for column_B in column_Bs:
   grouped_sum = V49_data.groupby(column_A)[column_B].sum()
   total_sum = V49_data[column_B].sum()
   column_B_percentage = grouped_sum / total_sum
   tax_composition[column_B] = column_B_percentage
pd.DataFrame(tax_composition).to_csv('taxon_composition.csv', sep=',', encoding='utf-8', index=True)  
# Calculate function_composition table
function_composition = pd.DataFrame()  
for column_B in column_Bs:
    grouped_sum = V49_data.groupby(['taxon', 'COG_number'])[column_B].sum()
    total_sum = V49_data.groupby(column_A)[column_B].sum()
    column_B_percentage = grouped_sum / total_sum
    function_composition[column_B] = column_B_percentage
# Calculate weighted_function_composition table
weighted_function_composition = pd.DataFrame()  
for column_B in column_Bs:
   grouped = V49_data.groupby(['taxon', 'COG_number'])[column_B].sum()
   total_sum = V49_data[column_B].sum()
   weighted_function_composition[column_B] = grouped / total_sum
pd.DataFrame(weighted_function_composition).to_csv('weighted_function_composition.csv', sep=',', encoding='utf-8', index=True) 

# Expand to represent all nodes up to the root of the phylogeny by summing up each node,
# Calculate extend_weighted_function_composition and weighted_function_composition_merge_all_nodes
# Convert the weighted function composition to percentage and create a table weighted_function_composition_percentage.csv
weighted_function_composition=pd.read_csv("weighted_function_composition.csv", sep = ',')
tree=rename_tree
def merge_weighted_function_composition_for_inner_nodes(clade):
    leaf_nodes = pd.DataFrame()
    if not clade.is_terminal():  
        for sub_clade in clade.clades:
            if sub_clade.is_terminal() and sub_clade.name!="None":  
                inner_node_results = pd.DataFrame(weighted_function_composition[weighted_function_composition['taxon'] == sub_clade.name][:])
                inner_node_results['taxon'] = clade.name 
                leaf_nodes = pd.concat([leaf_nodes, inner_node_results])
            else:
                if sub_clade.name!="None":
                    leaf_nodes = pd.concat([leaf_nodes, merge_weighted_function_composition_for_inner_nodes(sub_clade)])   # 递归查找子节点的叶子节点集合
        if not leaf_nodes.empty:  
            leaf_nodes.columns = weighted_function_composition.columns
    return leaf_nodes

column_Bs = V49_data.columns.tolist()[2:]
extend_weighted_function_composition=pd.DataFrame()

for clade in tree.find_clades():
    if not clade.is_terminal():
        leaf_set = merge_weighted_function_composition_for_inner_nodes(clade)
        if not leaf_set.empty: 
            group=pd.DataFrame()
            for column_B in column_Bs:
               group[column_B] = leaf_set.groupby(['COG_number'])[column_B].sum()
            group.insert(0, 'taxon', clade.name)
            extend_weighted_function_composition = pd.concat([extend_weighted_function_composition, group])

extend_weighted_function_composition.reset_index(inplace=True)
cols = extend_weighted_function_composition.columns.tolist()
cols = cols[1:2] + cols[0:1] + cols[2:]
extend_weighted_function_composition = extend_weighted_function_composition[cols]           
weighted_function_composition_all=pd.concat([weighted_function_composition, extend_weighted_function_composition])

# Convert the weighted_function_composition to percentage
weighted_function_composition_percentage = pd.DataFrame()  
for column_B in column_Bs:
    grouped_sum = weighted_function_composition_all.groupby(['taxon', 'COG_number'])[column_B].sum()
    total_sum = weighted_function_composition_all.groupby(column_A)[column_B].sum()
    column_B_percentage = grouped_sum / total_sum
    weighted_function_composition_percentage[column_B] = column_B_percentage
pd.DataFrame(weighted_function_composition_percentage).to_csv('weighted_function_composition_percentage.csv', sep=',', encoding='utf-8', index=True)  
weighted_function_composition_percentage=pd.read_csv('weighted_function_composition_percentage.csv', sep=',')  
taxon_ID = weighted_function_composition_percentage['taxon'].drop_duplicates().reset_index(drop=True)
for i in range(len(taxon_ID)):
    for column_B in column_Bs:
        flag=weighted_function_composition_percentage["taxon"]==taxon_ID[i]
        if weighted_function_composition_percentage.loc[flag, column_B].isna().all():
            weighted_function_composition_percentage.loc[flag, column_B]=weighted_function_composition_percentage.loc[flag, column_B].fillna(1/len(weighted_function_composition_percentage.loc[flag, column_B]))
pd.DataFrame(weighted_function_composition_percentage).to_csv('weighted_function_composition_percentage_fillna.csv', sep=',', encoding='utf-8', index=False)  
# Expand to represent all nodes up to the root of the phylogeny by summing up each node,
# Calculate extend_taxon_composition and extend_taxon_composition_merge_all_nodes table
taxon_composition=pd.read_csv("taxon_composition.csv", sep = ',')
def merge_taxon_composition_for_inner_nodes(clade):
    leaf_nodes = pd.DataFrame()
    if not clade.is_terminal():  
        for sub_clade in clade.clades:
            if sub_clade.is_terminal() and sub_clade.name!="None":  
                inner_node_results = pd.DataFrame(taxon_composition[taxon_composition['taxon'] == sub_clade.name][:])
                leaf_nodes = pd.concat([leaf_nodes,inner_node_results])
            else:
                if sub_clade.name!="None":
                    leaf_nodes = pd.concat([leaf_nodes, merge_taxon_composition_for_inner_nodes(sub_clade)])  
        if not leaf_nodes.empty:  
            leaf_nodes.columns = taxon_composition.columns
            leaf_nodes['taxon'] = clade.name  
    return leaf_nodes
# Generate extend_taxon_composition_merge_all_nodes table
extend_taxon_composition=pd.DataFrame()
for clade in tree.find_clades():
    if not clade.is_terminal():
        leaf_set = merge_taxon_composition_for_inner_nodes(clade)
        if not leaf_set.empty: 
            group = leaf_set.sum().to_frame().transpose()
            group["taxon"]=clade.name
            extend_taxon_composition = pd.concat([extend_taxon_composition, group], ignore_index=True)
extend_taxon_composition_merge_all_nodes=pd.concat([taxon_composition, extend_taxon_composition])
pd.DataFrame(extend_taxon_composition_merge_all_nodes).to_csv('extend_taxon_composition_merge_all_nodes.csv', sep=',', encoding='utf-8', index=False) 

# Step4: calculate PhyloFunc distance based on tables "weighted_function_composition_percentage" and "extend_taxon_composition_merge_all_nodes"
weighted_function_composition_percentage=pd.read_csv('weighted_function_composition_percentage_fillna.csv', sep=',')  
extend_taxon_composition_merge_all_nodes=pd.read_csv("extend_taxon_composition_merge_all_nodes.csv", sep = ',')
extend_taxon_composition_merge_all_nodes.iloc[:, [0,1]]
weight=pd.read_table('tree_rename.csv', sep = ',')
dab_matrix_norm = pd.DataFrame()
taxon_len=len(weighted_function_composition_percentage.index)
column_len=len(weighted_function_composition_percentage.columns)
for a in range(2, column_len):
    Ga_function = weighted_function_composition_percentage.iloc[:, [0,1,a]]
    Ga_taxon = extend_taxon_composition_merge_all_nodes.iloc[:, [0,a-1]]
    for b in range(2, column_len):
        Gb_function = weighted_function_composition_percentage.iloc[:, b]
        Gb_taxon = extend_taxon_composition_merge_all_nodes.iloc[:, b-1]
        Gab_function = pd.concat([Ga_function, Gb_function], axis=1)
        Gab_taxon = pd.concat([Ga_taxon, Gb_taxon], axis=1)
        Phylofunc=0
        first_column_value = weighted_function_composition_percentage['taxon'].unique()
        for t in first_column_value:
            #set the weight of root
            if t=='Node114':
                weight_taxon=1
            else:
                weight_taxon=weight.loc[weight['consequent']==t]['length'].iloc[0]
            data_cog_tax=Gab_function[(Gab_function["taxon"]==t)]
            origin_data_norm = data_cog_tax.iloc[:,2:].apply(lambda x: x/sum(x), axis = 0).fillna(0) 
            dab = 1 - sum(origin_data_norm.apply(lambda x: min(x), axis=1)) / sum(origin_data_norm.apply(lambda x: max(x), axis=1))
            a_abundance=Gab_taxon[(Gab_taxon["taxon"]==t)].iloc[0, 1] 
            b_abundance=Gab_taxon[(Gab_taxon["taxon"]==t)].iloc[0, 2] 
            Phylofunc=Phylofunc+dab*weight_taxon*a_abundance*b_abundance
        dab_matrix_norm.at[Gab_function.columns.values[2], Gab_function.columns.values[3]] = Phylofunc
        dab_matrix_norm.to_csv('PhyloFunc_distance.csv')
